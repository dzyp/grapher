# largest file sizes that can be upload
spring.servlet.multipart.max-file-size = 1000MB
spring.servlet.multipart.max-request-size = 1000MB

# Keep the connection alive if idle for a long time (needed in production)
spring.datasource.testWhileIdle = true
spring.datasource.validationQuery = SELECT 1

# Show or not log for each sql query
spring.jpa.show-sql = false

# Hibernate ddl auto (create, create-drop, update)
spring.jpa.hibernate.ddl-auto = validate

# Naming strategy
spring.jpa.hibernate.naming-strategy = org.hibernate.cfg.ImprovedNamingStrategy

# Use spring.jpa.properties.* for Hibernate native properties (the prefix is
# stripped before adding them to the entity manager)
# The SQL dialect makes Hibernate generate better SQL for the chosen database
spring.jpa.database-platform=org.hibernate.dialect.MySQL5InnoDBDialect

# Determines where logging messages are sent. The two options are "console" and "socket". Use console when running
# locally and socket when running deployed.
cerebral.logging = socket

# determines what tracing mechanism is going to be used. Options are 'discard', 'harbour', and 'console'. Discard will
# ignore all tracing information, console will log to stdout, and harbour will use wk's tracing implementation.
cerebral.tracer = harbour

# maximum lifetime of a temporary download tokenService in seconds
cerebral.maxTokenAgeInSeconds = 10

# Maximum number of bytes that a customer can upload. Like spring definitions, these accept human-readable definitions,
# like 10M or 100G.
cerebral.limits.lifetimeUploadLimit = 10000G

# Maximum number of bytes an account can scan in a single month. This can be defined in a human-readable byte form.
cerebral.limits.yearlyBytesScannedLimit = 100G
cerebral.permittedFileUploadMimeTypes=text/csv,application/json

## ADMIN CONFIGURATION
# name of the frugal service to broadcast
admin.frugalServiceName = cerebral-support-service

# defines the roles that can view the sql models
admin.supportRoles = super-admin|server-admin-viewer,super-admin|server-admin-software-support-engineer,super-admin|server-admin-rapid-response

# sets the root path of all requests. The value is derived from path-based routing rules created in Harbour.
server.servlet.context-path=/s/cerebral

# List of wdesk roles that are cerebral admins
cerebral.adminRoles=super-admin|server-admin-viewer,super-admin|server-admin-software-support-engineer,super-admin|server-admin-rapid-response,*|support-user,*|type-p-support-user,*|type-d-support-user,*|type-t-support-user,*|type-c-support-user,super-admin|super-admin
##########################################################################

# ANY PARAMETERS AFTER THIS POINT MUST BE SET SPECIFICALLY PER ENVIRONMENT
# AWS PARAMETERS
# Name of the import SQS queue
aws.importQueue=cerebral-prod-eu-west-1-import

# Name of the import dead letter SQS queue
aws.importDeadLetterQueue=cerebral-prod-eu-west-1-import-dead-letter

# Name of the query SQS queue
aws.queryQueue=cerebral-prod-eu-west-1-query

# Name of the query dead letter SQS queue
aws.queryDeadLetterQueue=cerebral-prod-eu-west-1-query-dead-letter

# Name of the SQS queue for long running resource-constrained jobs
aws.jobQueue=cerebral-prod-eu-west-1-job.fifo

# Name of the job dead letter queue
aws.jobDeadLetterQueue=cerebral-prod-eu-west-1-job-dead-letter.fifo

# For running async queries
aws.queryJobQueue=cerebral-prod-eu-west-1-query-job.fifo

# Recovers from queries that fail to start
aws.queryJobDeadLetterQueue=cerebral-prod-eu-west-1-query-job-dead-letter.fifo

# Access key should be the access key and secret key from the the user defined in the cloud formation template.
aws.accessKey = << filled in by rm >>
aws.secretKey = << filled in by rm >>

# The S3 bucket name as defined in the cloud formation parameters
aws.s3BucketName = cerebral-prod-eu-west-1

# This should be identical between environments, access is controlled by access key and secret key.
aws.athenaUrl = jdbc:awsathena://athena.eu-west-1.amazonaws.com:443

# This alias is defined in the cloud formation parameters.
aws.kmsAlias = alias/cerebral-encryption-eu

# Pretty self-explanatory, should also match the region defined in cloud formation template.
aws.region = eu-west-1

# name of glue job, found in cloudformation
aws.glueJobName=eu-cf-cerebral

# the number of cerebral glue jobs that exist in this environment
aws.numGlueJobs=3

# the number of workers each job can handle
aws.numWorkersPerJob=3

# END AWS PARAMETERS
# WDESK PARAMETERS

# The endpoint we should export CSVs to. This is a formatted string, the 1st value will be substituted with the
# workbook id, next is sheet id, last is a colon as required in the API documentation. The only thing that should
# really change here between environments is the host.
wdesk.baseSpreadsheetUrl = https://calc.eu.wdesk.com

# Defines the host for IAM services.
wdesk.iamHost = https://eu.wdesk.com

# Defines the client id when connecting to the above host. This plus the key should be generated the following way:
# openssl genrsa -out key.pem 4096
# openssl pkcs8 -topk8 -inform PEM -outform PEM -nocrypt -in key.pem -out key.pkcs8.pem
# The public key should be used in wdesk admin's oauth client definition. The resulting out file should be placed
# in the root directory of the service and should be named in the oauthPrivateKeyFile parameter.
wdesk.oauthClient = cerebral-client


# name of the cerebral application in the bigsky entitlement system
wdesk.applicationId=cerebral

# Described above.
wdesk.oauthPrivateKeyFile = key.pem

# For sending analytics to wdesk.
wdesk.analyticsUrl=https://analytics.eu.workiva.com/api/v1.0/analytics/
# END WDESK PARAMETERS

# SQL PARAMETERS
# The DSN for the master aurora instance (the write instance). An example:
# jdbc:mysql://<hostname>:3306/cerebral. Replace hostname with the endpoint from the RDS console.
spring.datasource.url = << filled in by rm >> -- be sure to add ?useSSL=true&requireSSL=true&verifyServerCertificate=true

# The username for the account to be used when connecting to the SQL database. The username is defined in the
# cloud formation parameters.
spring.datasource.username = cerebral

# This needs to be created and added to dynamo via the cloud deployer. The resulting value needs to be used in
# the cloud formation parameters and also placed here.
spring.datasource.password = << filled in by rm >>
# END SQL PARAMETERS

# REDIS PARAMETERS
spring.redis.host= << filled in by rm >>
spring.redis.password=<< filled in by rm >> 
spring.redis.ssl=true
# END REDIS PARAMETERS

# LIMITS
rate.limited.importFile.enabled=true
rate.limited.importFile.requests=5
rate.limited.importFile.interval=1
rate.limited.importFile.interval.unit=MINUTES
rate.limited.runQuery.enabled=true
rate.limited.runQuery.requests=5
rate.limited.runQuery.interval=1
rate.limited.runQuery.interval.unit=MINUTES
rate.limited.uploadFile.enabled=true
rate.limited.uploadFile.requests=5
rate.limited.uploadFile.interval=1
rate.limited.uploadFile.interval.unit=MINUTES
rate.limited.runPivot.enabled=true
rate.limited.runPivot.requests=1000000
rate.limited.runPivot.interval=1
rate.limited.runPivot.interval.unit=MINUTES
# END LIMITS

# CEREBRAL PARAMETERS
# Enables experimental features
cerebral.enableExperimentalFeatures=false

# Enables the checking of object-level permissions
cerebral.objectLevelPermissions=true

# Allowed CORS origins.
cerebral.allowedOrigins = https://h.eu.wdesk.com, http://eu.wdesk.com

# END CEREBRAL PARAMETERS

# MANAGEMENT PARAMETERS
# disable all management endpoints by default
management.endpoints.enabled-by-default=false

# END MANAGEMENT PARAMETERS

# DatasetService base url
workiva.datasetservice.baseUrl = https://h.eu.wdesk.com
